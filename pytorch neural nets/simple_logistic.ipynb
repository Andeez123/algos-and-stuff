{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2cc7593",
   "metadata": {},
   "source": [
    "Simple logistic regression with pytorch, using only one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e35e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import *\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import builtins\n",
    "range = builtins.range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be59dfc",
   "metadata": {},
   "source": [
    "Import the dataset from sklearn, and perform EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "164913d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c41c11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.shape) # data has 30 featues\n",
    "# print(y) # binary data of 0 and 1\n",
    "# print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9abeb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354d6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling training and testing data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.int64))\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b434d",
   "metadata": {},
   "source": [
    "PyTorch workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d574600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c08342dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device = {device}')\n",
    "model = LogisticRegression(n_features).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116a801",
   "metadata": {},
   "source": [
    "Loss and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab0b2878",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e8abdb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([455])\n",
      "torch.Size([455])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train.squeeze().long().shape)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edcdec69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss = 0.9396\n",
      "epoch: 2, loss = 0.8811\n",
      "epoch: 3, loss = 0.8283\n",
      "epoch: 4, loss = 0.7808\n",
      "epoch: 5, loss = 0.7381\n",
      "epoch: 6, loss = 0.6998\n",
      "epoch: 7, loss = 0.6654\n",
      "epoch: 8, loss = 0.6346\n",
      "epoch: 9, loss = 0.6068\n",
      "epoch: 10, loss = 0.5817\n",
      "epoch: 11, loss = 0.5590\n",
      "epoch: 12, loss = 0.5384\n",
      "epoch: 13, loss = 0.5196\n",
      "epoch: 14, loss = 0.5024\n",
      "epoch: 15, loss = 0.4866\n",
      "epoch: 16, loss = 0.4721\n",
      "epoch: 17, loss = 0.4586\n",
      "epoch: 18, loss = 0.4462\n",
      "epoch: 19, loss = 0.4346\n",
      "epoch: 20, loss = 0.4237\n",
      "epoch: 21, loss = 0.4136\n",
      "epoch: 22, loss = 0.4041\n",
      "epoch: 23, loss = 0.3952\n",
      "epoch: 24, loss = 0.3868\n",
      "epoch: 25, loss = 0.3788\n",
      "epoch: 26, loss = 0.3713\n",
      "epoch: 27, loss = 0.3642\n",
      "epoch: 28, loss = 0.3574\n",
      "epoch: 29, loss = 0.3510\n",
      "epoch: 30, loss = 0.3449\n",
      "epoch: 31, loss = 0.3390\n",
      "epoch: 32, loss = 0.3335\n",
      "epoch: 33, loss = 0.3281\n",
      "epoch: 34, loss = 0.3230\n",
      "epoch: 35, loss = 0.3181\n",
      "epoch: 36, loss = 0.3135\n",
      "epoch: 37, loss = 0.3090\n",
      "epoch: 38, loss = 0.3046\n",
      "epoch: 39, loss = 0.3005\n",
      "epoch: 40, loss = 0.2965\n",
      "epoch: 41, loss = 0.2926\n",
      "epoch: 42, loss = 0.2889\n",
      "epoch: 43, loss = 0.2853\n",
      "epoch: 44, loss = 0.2818\n",
      "epoch: 45, loss = 0.2784\n",
      "epoch: 46, loss = 0.2752\n",
      "epoch: 47, loss = 0.2720\n",
      "epoch: 48, loss = 0.2690\n",
      "epoch: 49, loss = 0.2660\n",
      "epoch: 50, loss = 0.2632\n",
      "epoch: 51, loss = 0.2604\n",
      "epoch: 52, loss = 0.2577\n",
      "epoch: 53, loss = 0.2550\n",
      "epoch: 54, loss = 0.2525\n",
      "epoch: 55, loss = 0.2500\n",
      "epoch: 56, loss = 0.2476\n",
      "epoch: 57, loss = 0.2453\n",
      "epoch: 58, loss = 0.2430\n",
      "epoch: 59, loss = 0.2408\n",
      "epoch: 60, loss = 0.2386\n",
      "epoch: 61, loss = 0.2365\n",
      "epoch: 62, loss = 0.2344\n",
      "epoch: 63, loss = 0.2324\n",
      "epoch: 64, loss = 0.2304\n",
      "epoch: 65, loss = 0.2285\n",
      "epoch: 66, loss = 0.2267\n",
      "epoch: 67, loss = 0.2248\n",
      "epoch: 68, loss = 0.2230\n",
      "epoch: 69, loss = 0.2213\n",
      "epoch: 70, loss = 0.2196\n",
      "epoch: 71, loss = 0.2179\n",
      "epoch: 72, loss = 0.2163\n",
      "epoch: 73, loss = 0.2147\n",
      "epoch: 74, loss = 0.2131\n",
      "epoch: 75, loss = 0.2116\n",
      "epoch: 76, loss = 0.2101\n",
      "epoch: 77, loss = 0.2086\n",
      "epoch: 78, loss = 0.2072\n",
      "epoch: 79, loss = 0.2058\n",
      "epoch: 80, loss = 0.2044\n",
      "epoch: 81, loss = 0.2030\n",
      "epoch: 82, loss = 0.2017\n",
      "epoch: 83, loss = 0.2004\n",
      "epoch: 84, loss = 0.1991\n",
      "epoch: 85, loss = 0.1979\n",
      "epoch: 86, loss = 0.1966\n",
      "epoch: 87, loss = 0.1954\n",
      "epoch: 88, loss = 0.1942\n",
      "epoch: 89, loss = 0.1931\n",
      "epoch: 90, loss = 0.1919\n",
      "epoch: 91, loss = 0.1908\n",
      "epoch: 92, loss = 0.1897\n",
      "epoch: 93, loss = 0.1886\n",
      "epoch: 94, loss = 0.1875\n",
      "epoch: 95, loss = 0.1865\n",
      "epoch: 96, loss = 0.1855\n",
      "epoch: 97, loss = 0.1844\n",
      "epoch: 98, loss = 0.1834\n",
      "epoch: 99, loss = 0.1825\n",
      "epoch: 100, loss = 0.1815\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    logits = model(X_train)\n",
    "\n",
    "    loss = loss_fn(logits, y_train.squeeze().long())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    #pytorch accumulates gradients by default, we have to reset it \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee5b1f",
   "metadata": {},
   "source": [
    "Evaluate the model on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58c60255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct = 105\n",
      "totals = 114\n",
      "accuracy = 92.11%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    logits = model(X_test)\n",
    "    y_predicted = torch.argmax(logits, dim = 1)\n",
    "\n",
    "    corrects = (y_predicted == y_test).sum().item()\n",
    "    print(f'correct = {corrects}')\n",
    "\n",
    "    totals = y_test.size(0)\n",
    "    print(f'totals = {totals}')\n",
    "\n",
    "    acc = corrects/totals\n",
    "    print(f'accuracy = {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "280bc81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer here\n",
    "class model_training():\n",
    "\n",
    "    def __init__(self, learning_rate: float, model: LogisticRegression, num_epoch: int, training_data, training_features, test_data, test_features):\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "        self.X_train = training_data\n",
    "        self.y_train = training_features\n",
    "\n",
    "        self.X_test = test_data\n",
    "        self.y_test = test_features\n",
    "\n",
    "        self.num_epochs = num_epoch\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "    def training_func(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.X_train, self.y_train = self.X_train.to(device), self.y_train.to(device) #load the data to device (GPU or CPU)\n",
    "            # compute logits using the model\n",
    "            logits = self.model(self.X_train)\n",
    "\n",
    "            loss = self.loss_fn(logits, self.y_train.squeeze().long())\n",
    "\n",
    "            # backward pass to compute the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # updates the model parameter based on the gradient\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "    \n",
    "    def evaluation_func(self):\n",
    "        # Ensure the model is in evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Load the data to the device (GPU or CPU)\n",
    "            self.X_test, self.y_test = self.X_test.to(device), self.y_test.to(device)\n",
    "            # Get the model's predictions\n",
    "            logits = self.model(self.X_test.type(torch.float32))\n",
    "            # Compute the predicted class\n",
    "            y_predicted = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # Calculate the number of correct predictions\n",
    "            corrects = (y_predicted == self.y_test).sum().item()\n",
    "            print(f'correct = {corrects}')\n",
    "\n",
    "            # Get the total number of samples\n",
    "            totals = self.y_test.size(0)\n",
    "            print(f'totals = {totals}')\n",
    "\n",
    "            # Compute the accuracy\n",
    "            acc = corrects / totals\n",
    "            print(f'accuracy = {acc * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e16d4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_model = LogisticRegression(n_features).to(device)\n",
    "trained_model = model_training(0.001, log_model, 30, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bd000c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss = 0.6017\n",
      "epoch: 2, loss = 0.5988\n",
      "epoch: 3, loss = 0.5959\n",
      "epoch: 4, loss = 0.5931\n",
      "epoch: 5, loss = 0.5903\n",
      "epoch: 6, loss = 0.5876\n",
      "epoch: 7, loss = 0.5848\n",
      "epoch: 8, loss = 0.5821\n",
      "epoch: 9, loss = 0.5795\n",
      "epoch: 10, loss = 0.5768\n",
      "epoch: 11, loss = 0.5742\n",
      "epoch: 12, loss = 0.5716\n",
      "epoch: 13, loss = 0.5691\n",
      "epoch: 14, loss = 0.5665\n",
      "epoch: 15, loss = 0.5640\n",
      "epoch: 16, loss = 0.5616\n",
      "epoch: 17, loss = 0.5591\n",
      "epoch: 18, loss = 0.5567\n",
      "epoch: 19, loss = 0.5543\n",
      "epoch: 20, loss = 0.5519\n",
      "epoch: 21, loss = 0.5496\n",
      "epoch: 22, loss = 0.5472\n",
      "epoch: 23, loss = 0.5449\n",
      "epoch: 24, loss = 0.5426\n",
      "epoch: 25, loss = 0.5404\n",
      "epoch: 26, loss = 0.5382\n",
      "epoch: 27, loss = 0.5359\n",
      "epoch: 28, loss = 0.5338\n",
      "epoch: 29, loss = 0.5316\n",
      "epoch: 30, loss = 0.5294\n"
     ]
    }
   ],
   "source": [
    "trained_model.training_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2cfa7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct = 83\n",
      "totals = 114\n",
      "accuracy = 72.81%\n"
     ]
    }
   ],
   "source": [
    "trained_model.evaluation_func()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
